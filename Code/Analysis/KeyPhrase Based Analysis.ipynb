{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0db3c5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd11d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "print(\"Current Directory\", path)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24ba762d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Samples: 161541\n"
     ]
    }
   ],
   "source": [
    "dataset_path = path + '/dataset/Full Dataset/'\n",
    "\n",
    "dataset_final = pd.read_json (dataset_path + 'dataset_total.json', encoding = 'utf-8')\n",
    "\n",
    "print('Total Samples:', len(dataset_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5be4251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_review_posted</th>\n",
       "      <th>user_total_helpful_votes</th>\n",
       "      <th>expertise</th>\n",
       "      <th>user_cities_visited</th>\n",
       "      <th>review_days</th>\n",
       "      <th>helpful_class</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34908</th>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>0.002587</td>\n",
       "      <td>102</td>\n",
       "      <td>0.200438</td>\n",
       "      <td>1</td>\n",
       "      <td>My family and I stayed for one night and were ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100113</th>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>42</td>\n",
       "      <td>0.065717</td>\n",
       "      <td>0</td>\n",
       "      <td>So, I parked my car at the double tree.  Gave ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1736</th>\n",
       "      <td>68</td>\n",
       "      <td>20</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>225</td>\n",
       "      <td>0.850493</td>\n",
       "      <td>0</td>\n",
       "      <td>I got a great deal to stay here &amp; although I h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32231</th>\n",
       "      <td>300</td>\n",
       "      <td>78</td>\n",
       "      <td>0.001152</td>\n",
       "      <td>41</td>\n",
       "      <td>0.401972</td>\n",
       "      <td>1</td>\n",
       "      <td>I stayed here for 10 nights and it became a ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21285</th>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.002587</td>\n",
       "      <td>5</td>\n",
       "      <td>0.457831</td>\n",
       "      <td>0</td>\n",
       "      <td>The Argonaut is well positioned on Fishermans ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_review_posted  user_total_helpful_votes  expertise  \\\n",
       "34908                   18                        10   0.002587   \n",
       "100113                  21                         4   0.000814   \n",
       "1736                    68                        20   0.001318   \n",
       "32231                  300                        78   0.001152   \n",
       "21285                    9                         5   0.002587   \n",
       "\n",
       "        user_cities_visited  review_days  helpful_class  \\\n",
       "34908                   102     0.200438              1   \n",
       "100113                   42     0.065717              0   \n",
       "1736                    225     0.850493              0   \n",
       "32231                    41     0.401972              1   \n",
       "21285                     5     0.457831              0   \n",
       "\n",
       "                                              review_text  \n",
       "34908   My family and I stayed for one night and were ...  \n",
       "100113  So, I parked my car at the double tree.  Gave ...  \n",
       "1736    I got a great deal to stay here & although I h...  \n",
       "32231   I stayed here for 10 nights and it became a ho...  \n",
       "21285   The Argonaut is well positioned on Fishermans ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dataset_final.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bc174a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "helpful_class\n",
       "0    101403\n",
       "1     47306\n",
       "2     11170\n",
       "3      1432\n",
       "4       230\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_final.groupby(['helpful_class']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d984652c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dataset_final[dataset_final.helpful_class == 0].sample(230)\n",
    "b = dataset_final[dataset_final.helpful_class == 1].sample(230)\n",
    "c = dataset_final[dataset_final.helpful_class == 2].sample(230)\n",
    "d = dataset_final[dataset_final.helpful_class == 3].sample(230)\n",
    "e = dataset_final[dataset_final.helpful_class == 4].sample(230)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf177a5",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3e473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18e40528",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_path = path + '/Lexicons/'\n",
    "\n",
    "def getStopWordSets():\n",
    "    # NLTK stopwords\n",
    "    stop_words = list(stopwords.words(\"english\"))\n",
    "    \n",
    "    stop_words.append('hotel')\n",
    "    stop_words.append('restaurant')\n",
    "    stop_words.append('night')\n",
    "    stop_words.append('day')\n",
    "    \n",
    "    # filter out the sentimental and emoticons from the keyphrases\n",
    "    df_lexicon = pd.read_csv(lexicon_path + 'vader_lexicon.txt', delimiter='\\t')\n",
    "    stop_words += list(df_lexicon['Lexicon'])\n",
    "    \n",
    "    df_emoji = pd.read_csv(lexicon_path + 'emoji_utf8_lexicon.txt', delimiter='\\t')\n",
    "    stop_words += list(df_emoji['Emotion'])\n",
    "    \n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9adf785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def extract_candidate_words(text):\n",
    "    # allowed POS tags\n",
    "    good_tags = set(['NN','NNS'])\n",
    "    \n",
    "    # exclude candidates that are stop words\n",
    "    stop_words = getStopWordSets()\n",
    "    \n",
    "    # tokenize and POS-tag words\n",
    "    tagged_words = itertools.chain.from_iterable(nltk.pos_tag_sents(nltk.word_tokenize(sent)\n",
    "                                                                   for sent in nltk.sent_tokenize(text)))\n",
    "    # filter on certain POS tags and lowercase all words\n",
    "    candidates = [word.lower() for word, tag in tagged_words\n",
    "                  if tag in good_tags and word.lower() not in stop_words\n",
    "                  and word.isalpha() and len(word) > 2]\n",
    "    candidates = [lemmatizer.lemmatize(word) for word in candidates]\n",
    "    candidates = [lemmatizer.lemmatize(word, pos='v') for word in candidates]\n",
    "    return \" \".join(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45951bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model input cartoon pixel caption model description image\n"
     ]
    }
   ],
   "source": [
    "print(extract_candidate_words(\"We investigate vision-and-language models that take as input the cartoon pixels and caption directly, as well as language-only models for which we circumvent image-processing by providing textual descriptions of the image.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b022577a",
   "metadata": {},
   "source": [
    "## Unigram, Bigram, and Trigram Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d8a117c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rightTypes(ngram):\n",
    "    if '-pron-' in ngram or '' in ngram or ' 'in ngram or 't' in ngram:\n",
    "        return False\n",
    "    for word in ngram:\n",
    "        if word in en_stopwords:\n",
    "            return False\n",
    "    acceptable_types = ('NN', 'NNS')\n",
    "    second_type = ('NN', 'NNS')\n",
    "    tags = nltk.pos_tag(ngram)\n",
    "    if tags[0][1] in acceptable_types and tags[1][1] in second_type:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    if ngram[0] == ngram[1]:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cc0524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to filter for trigrams\n",
    "def rightTypesTri(ngram):\n",
    "    if '-pron-' in ngram or '' in ngram or ' 'in ngram or 't' in ngram:\n",
    "        return False\n",
    "    for word in ngram:\n",
    "        if word in en_stopwords:\n",
    "            return False\n",
    "    first_type = ('NN', 'NNS')\n",
    "    third_type = ('NN', 'NNS')\n",
    "    tags = nltk.pos_tag(ngram)\n",
    "    if tags[0][1] in first_type and tags[2][1] in third_type:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44991d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigramFinder(filtered_words):\n",
    "    tokens = nltk.word_tokenize(filtered_words)\n",
    "    trigrams = nltk.collocations.TrigramAssocMeasures()\n",
    "    \n",
    "    trigramFinder = nltk.collocations.TrigramCollocationFinder.from_words(tokens)\n",
    "    \n",
    "    #bigramFinder.apply_freq_filter(20)\n",
    "    trigramLikTable = pd.DataFrame(list(trigramFinder.score_ngrams(trigrams.likelihood_ratio)), \n",
    "                                                   columns=['trigram','likelihood ratio']).sort_values(by='likelihood ratio', ascending=False)\n",
    "    filteredLik_tri = trigramLikTable[trigramLikTable.trigram.map(lambda x: rightTypesTri(x))]\n",
    "    filtered_frame = filteredLik_tri['trigram'][:10]\n",
    "  \n",
    "    aspects_list = filtered_frame.values.tolist()\n",
    "  \n",
    "    trigram_aspects_list = []\n",
    "    \n",
    "    for aspect in aspects_list:\n",
    "        trigram_aspects_list.append(aspect[0] + ' ' + aspect[1] + ' ' + aspect[2])\n",
    "    \n",
    "    return trigram_aspects_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fa1ff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigramFinder(filtered_words):\n",
    "    tokens = nltk.word_tokenize(filtered_words)\n",
    "    bigrams = nltk.collocations.BigramAssocMeasures()\n",
    "    \n",
    "    bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(tokens)\n",
    "    \n",
    "    #bigramFinder.apply_freq_filter(20)\n",
    "    bigramLikTable = pd.DataFrame(list(bigramFinder.score_ngrams(bigrams.likelihood_ratio)), \n",
    "                                              columns=['bigram','likelihood ratio']).sort_values(by='likelihood ratio', ascending=False)\n",
    "    filteredLik_bi = bigramLikTable[bigramLikTable.bigram.map(lambda x: rightTypes(x))]\n",
    "    filtered_frame = filteredLik_bi['bigram'][:10]\n",
    "  \n",
    "    aspects_list = filtered_frame.values.tolist()\n",
    "  \n",
    "    bigram_aspects_list = []\n",
    "    \n",
    "    for aspect in aspects_list:\n",
    "        bigram_aspects_list.append(aspect[0] + ' ' + aspect[1])\n",
    "    \n",
    "    return bigram_aspects_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcd9f60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigramFinder(filtered_words):\n",
    "    \n",
    "    tokens = nltk.word_tokenize(filtered_words)\n",
    "    \n",
    "    # Calculate frequency distribution\n",
    "    fdist = nltk.FreqDist(tokens)\n",
    "    \n",
    "    unigram_aspects_list = []\n",
    "    \n",
    "    # Output top 30 words\n",
    "    for word, frequency in fdist.most_common(20):\n",
    "        unigram_aspects_list.append(word)\n",
    "    \n",
    "    return unigram_aspects_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db752f6",
   "metadata": {},
   "source": [
    "## Find Top Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23bf8e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findTOPgrams(list_sentences):\n",
    "    \n",
    "    sentences = \" \".join(list_sentences)\n",
    "    filtered_words = extract_candidate_words(sentences)\n",
    "    \n",
    "    unigram_aspects = unigramFinder(filtered_words)\n",
    "    bigram_aspects = bigramFinder(filtered_words)\n",
    "    trigram_aspects = trigramFinder(filtered_words)\n",
    "    \n",
    "    return unigram_aspects, bigram_aspects, trigram_aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8d275ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS 0\n",
      "-------------------------------\n",
      "['room', 'staff', 'location', 'time', 'service', 'breakfast', 'stay', 'bed', 'pool', 'view', 'food', 'place', 'area', 'desk', 'front', 'bar', 'bathroom', 'night', 'floor', 'family']\n",
      "['front desk', 'coffee maker', 'breakfast buffet', 'sofa bed', 'swim pool', 'eye contact', 'make facility', 'valet park', 'partner bottom', 'cable car']\n",
      "['case front desk', 'front desk person', 'rate front desk', 'atrium front desk', 'front desk clerk', 'front desk costume', 'front desk lack', 'front desk monday', 'front desk personnel', 'door front desk']\n",
      "CLASS 1\n",
      "-------------------------------\n",
      "['room', 'staff', 'service', 'location', 'time', 'pool', 'breakfast', 'stay', 'bed', 'place', 'view', 'desk', 'food', 'price', 'front', 'bathroom', 'area', 'bar', 'floor', 'property']\n",
      "['front desk', 'shampoo conditioner', 'customer service', 'resort fee', 'pool area', 'park garage', 'coffee maker', 'king bed', 'skytrain station', 'bottle water']\n",
      "['service front desk', 'line front desk', 'availability front desk', 'front desk evacuation', 'front desk packet', 'front desk personnel', 'front desk professionalism', 'headache front desk', 'front desk concierge', 'front desk man']\n",
      "CLASS 2\n",
      "-------------------------------\n",
      "['room', 'staff', 'time', 'service', 'view', 'stay', 'desk', 'bed', 'pool', 'location', 'place', 'front', 'area', 'floor', 'beach', 'bathroom', 'breakfast', 'hour', 'property', 'card']\n",
      "['front desk', 'resort fee', 'customer service', 'coffee maker', 'city view', 'minute walk', 'cable car', 'shop restaurant', 'bottle water', 'travel agent']\n",
      "['front desk duty', 'cooky front desk', 'front desk agent', 'anything front desk', 'front desk girl', 'front desk clarification', 'front desk relay', 'front desk electronics', 'front desk duals', 'front desk conclusion']\n",
      "CLASS 3\n",
      "-------------------------------\n",
      "['room', 'staff', 'service', 'time', 'pool', 'stay', 'bed', 'area', 'desk', 'view', 'location', 'place', 'beach', 'floor', 'bar', 'breakfast', 'front', 'people', 'hotel', 'guest']\n",
      "['front desk', 'resort fee', 'customer service', 'minute walk', 'life jacket', 'taj taj', 'bottle water', 'bell desk', 'certificate brunch', 'grocery store']\n",
      "['front desk clerk', 'cart front desk', 'category front desk', 'front desk smell', 'front desk scorpion', 'front desk alternate', 'front desk remedy', 'guy front desk', 'front desk pic', 'front desk worker']\n",
      "CLASS 4\n",
      "-------------------------------\n",
      "['room', 'time', 'service', 'staff', 'pool', 'stay', 'area', 'bed', 'location', 'desk', 'beach', 'view', 'front', 'place', 'guest', 'hotel', 'thing', 'way', 'hour', 'floor']\n",
      "['front desk', 'resort fee', 'bed bug', 'beach chair', 'cable car', 'room rate', 'sale pitch', 'cancellation policy', 'customer service', 'knock knock']\n",
      "['front desk clerk', 'bug front desk', 'wife front desk', 'front desk manger', 'front desk staffer', 'front desk speak', 'front desk male', 'fax front desk', 'bellboy front desk', 'anyone front desk']\n"
     ]
    }
   ],
   "source": [
    "print(\"CLASS 0\")\n",
    "print(\"-------------------------------\")\n",
    "class_0_text = list(a['review_text'])\n",
    "\n",
    "unigram_aspects, bigram_aspects, trigram_aspects = findTOPgrams(class_0_text)\n",
    "\n",
    "print(unigram_aspects)\n",
    "print(bigram_aspects)\n",
    "print(trigram_aspects)\n",
    "\n",
    "print(\"CLASS 1\")\n",
    "print(\"-------------------------------\")\n",
    "class_1_text = list(b['review_text'])\n",
    "\n",
    "unigram_aspects, bigram_aspects, trigram_aspects = findTOPgrams(class_1_text)\n",
    "\n",
    "print(unigram_aspects)\n",
    "print(bigram_aspects)\n",
    "print(trigram_aspects)\n",
    "\n",
    "print(\"CLASS 2\")\n",
    "print(\"-------------------------------\")\n",
    "class_2_text = list(c['review_text'])\n",
    "\n",
    "unigram_aspects, bigram_aspects, trigram_aspects = findTOPgrams(class_2_text)\n",
    "\n",
    "print(unigram_aspects)\n",
    "print(bigram_aspects)\n",
    "print(trigram_aspects)\n",
    "\n",
    "print(\"CLASS 3\")\n",
    "print(\"-------------------------------\")\n",
    "class_3_text = list(d['review_text'])\n",
    "\n",
    "unigram_aspects, bigram_aspects, trigram_aspects = findTOPgrams(class_3_text)\n",
    "\n",
    "print(unigram_aspects)\n",
    "print(bigram_aspects)\n",
    "print(trigram_aspects)\n",
    "\n",
    "print(\"CLASS 4\")\n",
    "print(\"-------------------------------\")\n",
    "class_4_text = list(e['review_text'])\n",
    "\n",
    "unigram_aspects, bigram_aspects, trigram_aspects = findTOPgrams(class_4_text)\n",
    "\n",
    "print(unigram_aspects)\n",
    "print(bigram_aspects)\n",
    "print(trigram_aspects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cf8191",
   "metadata": {},
   "source": [
    "## Save the Analysis Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d77ad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db5aae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_frame = pd.concat([a, b, c, d, e], axis=0)\n",
    "\n",
    "with open(dataset_path + \"analysis_final.json\", 'w', encoding='utf-8') as file:\n",
    "    file.write(json.dumps(full_data_frame.to_dict('records'), indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
