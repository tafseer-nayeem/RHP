{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58831f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8ca06f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "#torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f055f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# https://huggingface.co/activebus/BERT-XD_Review\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"activebus/BERT-XD_Review\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea84edcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bba8d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'how', 'are', 'you', '?']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize('Hello WORLD how ARE yoU?')\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29000797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7592, 2088, 2129, 2024, 2017, 1029]\n"
     ]
    }
   ],
   "source": [
    "indexes = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b219f07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] [SEP] [PAD] [UNK]\n"
     ]
    }
   ],
   "source": [
    "init_token = tokenizer.cls_token\n",
    "eos_token = tokenizer.sep_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "\n",
    "print(init_token, eos_token, pad_token, unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acfb8b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 102 0 100\n"
     ]
    }
   ],
   "source": [
    "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e25fb96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 102 0 100\n"
     ]
    }
   ],
   "source": [
    "init_token_idx = tokenizer.cls_token_id\n",
    "eos_token_idx = tokenizer.sep_token_id\n",
    "pad_token_idx = tokenizer.pad_token_id\n",
    "unk_token_idx = tokenizer.unk_token_id\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a07551d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
    "\n",
    "print(max_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a63f5a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_cut(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50e3d2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy import data\n",
    "from torchtext.legacy import datasets\n",
    "\n",
    "\n",
    "E = data.LabelField(dtype = torch.float, batch_first=True, sequential=False, use_vocab=False) # The Expertise\n",
    "\n",
    "D = data.LabelField(dtype = torch.float, batch_first=True, sequential=False, use_vocab=False) # The Review Age in Days\n",
    "\n",
    "\n",
    "TEXT = data.Field(batch_first=True, use_vocab = False, include_lengths = True,\n",
    "                      tokenize = tokenize_and_cut, preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                      init_token = init_token_idx, eos_token = eos_token_idx,\n",
    "                      pad_token = pad_token_idx, unk_token = unk_token_idx)  # Review Text\n",
    "\n",
    "LABEL = data.LabelField(dtype = torch.long, batch_first=True, sequential=False, use_vocab=False) # Helpfulness Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adeb2525",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = {'expertise': ('e', E), 'review_days': ('d', D), 'review_text': ('text', TEXT), 'helpful_class': ('label', LABEL)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e700dbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = './dataset/Experiment/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3474eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
    "                                        path = root_path,\n",
    "                                        train = 'train.json',\n",
    "                                        validation = 'valid.json',\n",
    "                                        test = 'test.json',\n",
    "                                        format = 'json',\n",
    "                                        fields = fields\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d858bf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 145381\n",
      "Number of validation examples: 8080\n",
      "Number of testing examples: 8080\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data)}\")\n",
    "print(f\"Number of testing examples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2649356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'e': 0.002317113538563, 'd': 0.9611171960569551, 'text': [4370, 2045, 1037, 2261, 2335, 2023, 100, 2004, 1037, 6976, 10367, 2000, 16420, 1998, 100, 2940, 1045, 2293, 2129, 2673, 2055, 2023, 2173, 2003, 100, 1996, 100, 1996, 6912, 2686, 1998, 1996, 100, 1045, 2034, 2234, 2000, 2131, 2185, 2013, 1996, 9045, 1997, 2586, 100, 1998, 4370, 2005, 2129, 2012, 2188, 2009, 2081, 100, 100, 2057, 5247, 1037, 2843, 1997, 2051, 1999, 1996, 100, 100, 2551, 100, 100, 2096, 2108, 5845, 2066, 16664, 2011, 1996, 8422, 2457, 2136, 100, 100, 14736, 1998, 1037, 3626, 1997, 5541, 1998, 2326, 8048, 3347, 100, 1045, 16755, 2023, 100], 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fb971c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'e': 0.001458168886172, 'd': 0.6067907995618831, 'text': [1045, 4370, 2182, 1999, 2249, 2307, 3942, 3866, 1996, 100, 3435, 2830, 1020, 2086, 17829, 2081, 1017, 2706, 3805, 1997, 100, 4484, 2068, 1016, 2335, 2007, 2070, 5379, 2111, 2006, 1996, 100, 3198, 2065, 2057, 2071, 2681, 2256, 8641, 2007, 100, 2065, 1996, 2282, 100, 3201, 2007, 1996, 2111, 2006, 1996, 100, 2053, 3291, 1045, 2001, 2409, 2021, 2175, 3805, 1998, 3198, 2068, 2065, 2115, 2282, 2003, 3201, 2065, 2009, 2003, 2027, 2097, 2292, 2017, 4638, 100, 2588, 5508, 1045, 2175, 2000, 1996, 4624, 2025, 8074, 2000, 2022, 7039, 1999, 1998, 1996, 2611, 2758, 2200, 100, 2008, 2009, 2001, 2000, 2220, 2005, 4638, 100, 2029, 1045, 4415, 5319, 1998, 2074, 2359, 2000, 4638, 2026, 8641, 2061, 2057, 2071, 2707, 4356, 100, 2053, 2342, 2005, 100, 2027, 2435, 2033, 1037, 2367, 2561, 2084, 2054, 1045, 2001, 2409, 2006, 1996, 3042, 2061, 1045, 3092, 2039, 7079, 2062, 2005, 1037, 5958, 2305, 2282, 2084, 1045, 2323, 100, 1045, 2001, 2025, 1999, 1996, 6888, 2000, 7475, 2007, 3087, 2061, 2057, 9411, 2007, 100, 2131, 1999, 1996, 2282, 1998, 1045, 2001, 2200, 9364, 2009, 2074, 3849, 2008, 1999, 1020, 2086, 2009, 2038, 2908, 2091, 100, 2153, 2057, 9411, 2007, 100, 1045, 2018, 2070, 2204, 6735, 2007, 1996, 19832, 1998, 2057, 2209, 1037, 2210, 2012, 1996, 7251, 2028, 2305, 1045, 2001, 2025, 7622, 2007, 1996, 6770, 5126, 4496, 1996, 100, 1997, 1996, 100, 2006, 1996, 2197, 2154, 2057, 2106, 2031, 2019, 5665, 10367, 2007, 2149, 1998, 2027, 3724, 4638, 2041, 2067, 2019, 3178, 2061, 2016, 2071, 2717, 100, 2016, 2979, 2041, 1999, 1996, 9270, 1998, 1996, 3036, 3095, 2499, 2855, 2000, 6509, 2149, 1999, 2893, 2014, 4201, 2091, 1998, 2580, 1037, 13443, 2000, 2562, 100, 2013, 100, 2027, 2170, 100, 1998, 2016, 6757, 2007, 20989, 1998, 1037, 2261, 2420, 1997, 2717, 2012, 100, 2008, 2588, 2023, 3942, 2003, 13718, 1996, 2069, 8489, 1045, 2097, 2507, 2023, 100, 2197, 100, 2009, 2003, 2200, 5189, 2012, 2305, 2021, 2008, 2003, 2000, 2022, 3517, 2065, 2017, 2024, 6595, 2006, 22550, 2009, 2788, 100, 2091, 2105, 1016, 2030, 1017, 100, 1996, 5005, 100, 8572, 2033, 2138, 1045, 2994, 5116, 2138, 1997, 1996, 100], 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "#print text\n",
    "print(vars(train_data.examples[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22678a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'stayed', 'here', 'in', '2011', 'great', 'visit', 'loved', 'the', '[UNK]', 'fast', 'forward', '6', 'years', 'reservations', 'made', '3', 'months', 'ahead', 'of', '[UNK]', 'confirmed', 'them', '2', 'times', 'with', 'some', 'friendly', 'people', 'on', 'the', '[UNK]', 'ask', 'if', 'we', 'could', 'leave', 'our', 'bags', 'with', '[UNK]', 'if', 'the', 'room', '[UNK]', 'ready', 'with', 'the', 'people', 'on', 'the', '[UNK]', 'no', 'problem', 'i', 'was', 'told', 'but', 'go', 'ahead', 'and', 'ask', 'them', 'if', 'your', 'room', 'is', 'ready', 'if', 'it', 'is', 'they', 'will', 'let', 'you', 'check', '[UNK]', 'upon', 'arrival', 'i', 'go', 'to', 'the', 'desk', 'not', 'expecting', 'to', 'be', 'checked', 'in', 'and', 'the', 'girl', 'says', 'very', '[UNK]', 'that', 'it', 'was', 'to', 'early', 'for', 'check', '[UNK]', 'which', 'i', 'clearly', 'understood', 'and', 'just', 'wanted', 'to', 'check', 'my', 'bags', 'so', 'we', 'could', 'start', 'sight', '[UNK]', 'no', 'need', 'for', '[UNK]', 'they', 'gave', 'me', 'a', 'different', 'total', 'than', 'what', 'i', 'was', 'told', 'on', 'the', 'phone', 'so', 'i', 'ended', 'up', 'paying', 'more', 'for', 'a', 'friday', 'night', 'room', 'than', 'i', 'should', '[UNK]', 'i', 'was', 'not', 'in', 'the', 'mood', 'to', 'argue', 'with', 'anyone', 'so', 'we', 'dealt', 'with', '[UNK]', 'get', 'in', 'the', 'room', 'and', 'i', 'was', 'very', 'disappointed', 'it', 'just', 'seems', 'that', 'in', '6', 'years', 'it', 'has', 'gone', 'down', '[UNK]', 'again', 'we', 'dealt', 'with', '[UNK]', 'i', 'had', 'some', 'good', 'luck', 'with', 'the', 'slots', 'and', 'we', 'played', 'a', 'little', 'at', 'the', 'tables', 'one', 'night', 'i', 'was', 'not', 'impressed', 'with', 'the', 'pit', 'employees', 'nor', 'the', '[UNK]', 'of', 'the', '[UNK]', 'on', 'the', 'last', 'day', 'we', 'did', 'have', 'an', 'ill', 'visitor', 'with', 'us', 'and', 'they', 'pushed', 'check', 'out', 'back', 'an', 'hour', 'so', 'she', 'could', 'rest', '[UNK]', 'she', 'passed', 'out', 'in', 'the', 'casino', 'and', 'the', 'security', 'staff', 'worked', 'quickly', 'to', 'assist', 'us', 'in', 'getting', 'her', 'laid', 'down', 'and', 'created', 'a', 'perimeter', 'to', 'keep', '[UNK]', 'from', '[UNK]', 'they', 'called', '[UNK]', 'and', 'she', 'recovered', 'with', 'fluids', 'and', 'a', 'few', 'days', 'of', 'rest', 'at', '[UNK]', 'that', 'upon', 'this', 'visit', 'is', 'sadly', 'the', 'only', 'praise', 'i', 'will', 'give', 'this', '[UNK]', 'last', '[UNK]', 'it', 'is', 'very', 'loud', 'at', 'night', 'but', 'that', 'is', 'to', 'be', 'expected', 'if', 'you', 'are', 'staying', 'on', 'fremont', 'it', 'usually', '[UNK]', 'down', 'around', '2', 'or', '3', '[UNK]', 'the', 'noise', '[UNK]', 'bother', 'me', 'because', 'i', 'stay', 'downtown', 'because', 'of', 'the', '[UNK]']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[2])['text'])\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91ecf4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "E.build_vocab(train_data)\n",
    "D.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86549416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {0: 0, 1: 1, 2: 2, 3: 3, 4: 4})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2d268d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    sort_key = lambda x: len(x.text),\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "776a408d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8591fd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created `torchtext_train_dataloader` with 4544 batches!\n",
      "Created `torchtext_valid_dataloader` with 253 batches!\n",
      "Created `torchtext_test_dataloader` with 253 batches!\n"
     ]
    }
   ],
   "source": [
    "# Print number of batches in each split\n",
    "print('Created `torchtext_train_dataloader` with %d batches!'%len(train_iterator))\n",
    "print('Created `torchtext_valid_dataloader` with %d batches!'%len(valid_iterator))\n",
    "print('Created `torchtext_test_dataloader` with %d batches!'%len(test_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "041dd909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorchText BuketIterator\n",
      "\n",
      "Batch size: 32\n",
      "\n",
      "1\t19 \t \t [5723, 100, 2053, 2980, 2300, 1999, 1996, 2851, 2000, 100, 2027, 2342, 2000, 4550, 2488, 1998, 2131, 2047, 100]\n",
      "0\t20 \t \t [3835, 3309, 2007, 2200, 3835, 5379, 14044, 100, 1996, 2069, 2613, 12087, 2001, 4026, 100, 2060, 2084, 2008, 2307, 100]\n",
      "0\t25 \t \t [2307, 2131, 2185, 1010, 2307, 2833, 5379, 1998, 14044, 100, 2307, 3295, 3733, 3229, 2000, 6023, 1998, 7419, 100, 6581, 2097, 2272, 2067, 2293, 100]\n",
      "1\t25 \t \t [2753, 100, 5186, 2502, 100, 100, 1998, 6625, 9705, 2007, 3528, 1997, 100, 100, 2379, 6005, 1998, 4500, 2000, 1996, 4680, 100, 100, 100, 6749]\n",
      "0\t26 \t \t [4283, 1011, 2428, 3835, 2173, 1999, 1037, 2307, 100, 1045, 2074, 4299, 2027, 2018, 18833, 8831, 2000, 3715, 2026, 18059, 2144, 1045, 9471, 1996, 2813, 100]\n",
      "0\t26 \t \t [2200, 3835, 3295, 2485, 2000, 1996, 100, 1999, 2001, 100, 2001, 100, 2003, 100, 2003, 100, 100, 9726, 1998, 5742, 100, 100, 2041, 2001, 100, 100]\n",
      "0\t27 \t \t [2200, 3835, 1998, 4550, 100, 3835, 2715, 1998, 2152, 7216, 100, 100, 4770, 1998, 10516, 100, 11549, 100, 4825, 2012, 4189, 100, 3295, 2485, 2000, 2270, 100]\n",
      "0\t28 \t \t [100, 100, 24541, 5053, 11090, 1999, 2900, 2038, 2441, 2012, 1996, 100, 3309, 100, 100, 100, 5053, 11090, 2007, 1037, 2381, 1997, 2058, 2531, 2086, 2012, 100, 100]\n",
      "0\t28 \t \t [3835, 4734, 1998, 2307, 100, 4511, 3178, 2012, 1996, 100, 3835, 3626, 1998, 2200, 100, 4734, 2021, 15708, 1998, 2200, 100, 2003, 2036, 100, 3788, 3292, 2013, 100]\n",
      "0\t29 \t \t [2023, 3309, 2003, 2012, 1037, 2204, 100, 2049, 16286, 2485, 2000, 6167, 5869, 7136, 100, 2049, 1037, 14901, 100, 1006, 100, 2005, 2026, 2394, 1045, 2572, 2145, 4083, 1007]\n",
      "0\t29 \t \t [2307, 4734, 2005, 100, 100, 100, 2000, 10798, 1998, 100, 1998, 6023, 100, 3095, 1998, 100, 100, 7628, 1998, 9318, 100, 6350, 2007, 6429, 100, 1998, 15736, 2800, 2036]\n",
      "0\t29 \t \t [6581, 100, 3495, 2408, 2013, 1996, 100, 17950, 3591, 1998, 2296, 2282, 19532, 1998, 24820, 2000, 1996, 100, 2467, 100, 3819, 100, 5379, 3095, 100, 5621, 100, 100, 100]\n",
      "0\t29 \t \t [2428, 1037, 3835, 3309, 2000, 2994, 2593, 2309, 2030, 2007, 100, 22445, 100, 100, 3095, 100, 100, 5632, 2026, 100, 2064, 16755, 2023, 100, 2000, 100, 2276, 2003, 2379]\n",
      "0\t29 \t \t [2179, 1996, 5723, 7497, 1037, 2210, 100, 2084, 2023, 2673, 2001, 100, 2001, 100, 2001, 100, 3788, 3292, 2013, 5536, 2538, 1998, 100, 100, 2020, 2200, 5379, 1998, 14044]\n",
      "1\t29 \t \t [6429, 3325, 1011, 3376, 100, 100, 3819, 2326, 1011, 1996, 3095, 2020, 2034, 100, 6350, 4128, 1999, 1996, 100, 2465, 12679, 2007, 23273, 100, 2578, 2000, 7884, 2003, 3819]\n",
      "0\t29 \t \t [3295, 3295, 100, 4734, 100, 1998, 3034, 2415, 2020, 100, 1996, 100, 2408, 1996, 2395, 2003, 100, 1998, 3138, 2729, 1997, 2035, 1996, 6827, 100, 3791, 2926, 26071, 100]\n",
      "0\t30 \t \t [4734, 100, 2307, 100, 3835, 100, 3295, 4500, 1996, 2047, 2248, 18086, 100, 4770, 100, 6581, 3095, 2040, 2024, 2200, 100, 27902, 3277, 3791, 3319, 2021, 3452, 2994, 2200, 100]\n",
      "0\t30 \t \t [2673, 2003, 100, 2498, 100, 2986, 7759, 1999, 100, 2021, 2116, 3601, 7884, 100, 2152, 8158, 2007, 2300, 5328, 2024, 100, 9004, 5379, 1998, 6020, 100, 2017, 100, 2022, 100]\n",
      "1\t30 \t \t [2673, 2003, 2327, 100, 100, 2000, 18066, 2270, 100, 2000, 100, 1998, 6625, 100, 2000, 2019, 6429, 6350, 2007, 1037, 100, 2000, 2489, 100, 2000, 10382, 2326, 100, 2022, 100]\n",
      "0\t30 \t \t [4370, 2182, 2005, 100, 2307, 8013, 2326, 6429, 100, 1996, 3124, 2551, 2182, 2001, 3565, 5379, 1998, 2507, 5151, 100, 2282, 2003, 2200, 4550, 2200, 100, 2052, 2272, 2067, 100]\n",
      "0\t30 \t \t [2307, 100, 3376, 2715, 3309, 2007, 3518, 100, 5379, 3095, 1998, 3452, 3376, 100, 3811, 16755, 2023, 100, 2009, 2001, 1996, 2190, 100, 2057, 2179, 1999, 100, 3811, 16755, 100]\n",
      "0\t30 \t \t [2485, 2000, 6023, 2181, 1998, 100, 2092, 4198, 2000, 100, 3835, 100, 100, 4550, 100, 2006, 19227, 2057, 2020, 10979, 2007, 1037, 5835, 1997, 16619, 100, 2785, 1998, 5629, 100]\n",
      "0\t30 \t \t [2307, 100, 6919, 2326, 1998, 100, 6530, 10427, 4825, 1013, 3347, 2200, 4658, 100, 2307, 100, 3243, 1037, 100, 2200, 4550, 1998, 3824, 2200, 6018, 100, 1999, 2035, 2307, 3325]\n",
      "0\t30 \t \t [2307, 100, 4370, 1018, 6385, 1999, 1996, 100, 2307, 2005, 100, 2542, 2282, 2181, 1998, 5010, 5459, 2011, 4979, 100, 10382, 2326, 1998, 100, 1996, 5851, 5581, 100, 2994, 100]\n",
      "0\t30 \t \t [100, 5379, 1998, 14044, 100, 2898, 6350, 100, 2282, 2001, 2682, 100, 6457, 2001, 100, 2204, 3295, 2012, 100, 100, 3452, 2009, 2001, 1037, 2200, 6625, 100, 2097, 2709, 100]\n",
      "2\t30 \t \t [7078, 12090, 18081, 6097, 1009, 8808, 1998, 1037, 100, 4138, 7967, 100, 2007, 5328, 1997, 5862, 100, 2026, 100, 100, 2001, 100, 1998, 3407, 2000, 2191, 11433, 1011, 12246, 100]\n",
      "1\t30 \t \t [2204, 2173, 2200, 2379, 2000, 6023, 3182, 2066, 2586, 2675, 1998, 100, 22445, 100, 2366, 4840, 6350, 1998, 5379, 100, 100, 7906, 4734, 29369, 1998, 100, 2052, 16755, 2000, 100]\n",
      "1\t30 \t \t [2307, 3295, 2000, 6005, 100, 14044, 100, 2307, 3295, 2005, 2449, 6295, 2030, 100, 100, 100, 5592, 4497, 2038, 2307, 100, 14057, 29500, 2408, 1996, 100, 4734, 4550, 1998, 100]\n",
      "0\t30 \t \t [2057, 4669, 2673, 2055, 2023, 100, 1996, 3295, 2001, 100, 2379, 2204, 100, 6023, 1998, 2270, 100, 2326, 2001, 2200, 5379, 1998, 100, 2007, 1996, 3095, 4346, 6581, 7759, 100]\n",
      "0\t30 \t \t [1996, 22583, 9471, 2000, 3073, 7815, 2076, 1996, 100, 2053, 2606, 100, 11868, 8248, 1004, 100, 100, 100, 3643, 2005, 2769, 1998, 6143, 100, 3788, 3292, 2000, 100, 100, 100]\n",
      "0\t31 \t \t [2204, 3295, 2005, 11131, 1996, 100, 3835, 100, 6581, 100, 1996, 3095, 2020, 11757, 100, 1004, 100, 100, 3958, 6749, 2070, 2204, 7884, 1998, 2001, 2200, 14044, 1998, 5379, 1999, 100]\n",
      "0\t31 \t \t [6581, 2005, 2460, 2744, 2994, 2007, 2307, 2334, 2833, 2485, 100, 3095, 2020, 2036, 5186, 100, 1998, 100, 100, 2019, 100, 2021, 2898, 2846, 1997, 100, 2092, 21125, 100, 4651, 100]\n",
      "\n",
      "\n",
      "19 \t 0.009599 \t 0.292990 \n",
      "20 \t 0.004745 \t 0.026835 \n",
      "25 \t 0.002317 \t 0.149507 \n",
      "25 \t 0.009599 \t 0.786418 \n",
      "26 \t 0.001508 \t 0.298467 \n",
      "26 \t 0.001554 \t 0.506572 \n",
      "27 \t 0.004745 \t 0.919496 \n",
      "28 \t 0.004745 \t 0.642388 \n",
      "28 \t 0.001832 \t 0.442497 \n",
      "29 \t 0.004745 \t 0.279847 \n",
      "29 \t 0.002317 \t 0.698248 \n",
      "29 \t 0.004745 \t 0.725082 \n",
      "29 \t 0.001508 \t 0.474808 \n",
      "29 \t 0.004745 \t 0.685104 \n",
      "29 \t 0.003126 \t 0.906900 \n",
      "29 \t 0.001508 \t 0.722344 \n",
      "30 \t 0.001277 \t 0.335706 \n",
      "30 \t 0.001214 \t 0.458379 \n",
      "30 \t 0.007172 \t 0.173604 \n",
      "30 \t 0.004745 \t 0.843921 \n",
      "30 \t 0.004745 \t 0.842826 \n",
      "30 \t 0.001832 \t 0.350493 \n",
      "30 \t 0.004745 \t 0.309967 \n",
      "30 \t 0.004745 \t 0.866375 \n",
      "30 \t 0.003126 \t 0.101862 \n",
      "30 \t 0.001407 \t 0.239869 \n",
      "30 \t 0.002317 \t 0.081599 \n",
      "30 \t 0.009599 \t 0.373494 \n",
      "30 \t 0.001277 \t 0.680723 \n",
      "30 \t 0.001508 \t 0.066813 \n",
      "31 \t 0.003126 \t 0.876232 \n",
      "31 \t 0.004745 \t 0.366375 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create batches - needs to be called before each loop.\n",
    "test_iterator.create_batches()\n",
    "\n",
    "# Loop through BucketIterator.\n",
    "print('PyTorchText BuketIterator\\n')\n",
    "for batch in test_iterator.batches:\n",
    "\n",
    "  # Let's check batch size.\n",
    "  print('Batch size: %d\\n'% len(batch))\n",
    "  #print('LABEL\\tLENGTH\\tTEXT'.ljust(5))\n",
    "\n",
    "  # Print each example.\n",
    "  for example in batch:\n",
    "    print('%s\\t%d \\t \\t %s'.ljust(10) % (example.label, len(example.text), example.text))\n",
    "  print('\\n')\n",
    "\n",
    "    # Print each example.\n",
    "  for example in batch:\n",
    "    print('%d \\t %f \\t %f '.ljust(10) % (len(example.text), example.e, example.d))\n",
    "  print('\\n')\n",
    "\n",
    "  # Only look at first batch. Reuse this code in training models.\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e992edd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at activebus/BERT-XD_Review were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/activebus/BERT-XD_Review\n",
    "bert = AutoModel.from_pretrained(\"activebus/BERT-XD_Review\")\n",
    "\n",
    "print(bert.config.to_dict()['hidden_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "daf6301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BERTHelpful(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        \n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        \n",
    "        self.mlp_bert = nn.Linear(embedding_dim, hidden_dim)\n",
    "        \n",
    "        self.mlp_len = nn.Linear(1, 1)\n",
    "        \n",
    "        self.mlp_days = nn.Linear(1, 1)\n",
    "        \n",
    "        self.mlp_exp = nn.Linear(1, 1)\n",
    "        \n",
    "        self.out = nn.Linear(hidden_dim + 3, output_dim)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, text_lengths, exp, days):\n",
    "        \n",
    "        #text = [batch size, sent len]\n",
    "        \n",
    "        # Feed input to BERT\n",
    "        with torch.no_grad():\n",
    "            embedded = self.bert(text)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        hidden = embedded[0][:, 0, :]\n",
    "        \n",
    "        # https://discuss.huggingface.co/t/what-is-the-purpose-of-the-additional-dense-layer-in-classification-heads/526\n",
    "        hidden = self.dropout(hidden)\n",
    "        \n",
    "        intermediate = self.relu(self.mlp_bert(hidden))\n",
    "        \n",
    "        reshaped_days = days.view(len(days), 1)\n",
    "        weights_days = self.mlp_days(reshaped_days)\n",
    "        \n",
    "        reshaped_exp = exp.view(len(exp), 1)\n",
    "        weights_exp = self.mlp_exp(reshaped_exp)\n",
    "        \n",
    "        # https://www.tutorialspoint.com/how-to-perform-element-wise-division-on-tensors-in-pytorch\n",
    "        reshaped_len = text_lengths.view(len(days), 1)\n",
    "        norm_length = torch.div(reshaped_len, 512)\n",
    "        weights_len = self.mlp_len(norm_length)\n",
    "        \n",
    "        fusion = torch.cat((weights_days, intermediate), dim = 1)\n",
    "        \n",
    "        fusion = torch.cat((weights_exp, fusion), dim = 1)\n",
    "        \n",
    "        fusion = torch.cat((weights_len, fusion), dim = 1)\n",
    "        \n",
    "        output = self.out(fusion)\n",
    "        \n",
    "        #output = [batch size, out dim]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "494cb712",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 50\n",
    "OUTPUT_DIM = len(LABEL.vocab)\n",
    "DROPOUT = 0.2\n",
    "\n",
    "N_EPOCHS = 5\n",
    "\n",
    "model = BERTHelpful(bert,\n",
    "                         HIDDEN_DIM,\n",
    "                         OUTPUT_DIM,\n",
    "                         DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53f04509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 109,520,966 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a214edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():                \n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a64ca76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 109,520,966 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "902d1a5b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight\n",
      "bert.embeddings.position_embeddings.weight\n",
      "bert.embeddings.token_type_embeddings.weight\n",
      "bert.embeddings.LayerNorm.weight\n",
      "bert.embeddings.LayerNorm.bias\n",
      "bert.encoder.layer.0.attention.self.query.weight\n",
      "bert.encoder.layer.0.attention.self.query.bias\n",
      "bert.encoder.layer.0.attention.self.key.weight\n",
      "bert.encoder.layer.0.attention.self.key.bias\n",
      "bert.encoder.layer.0.attention.self.value.weight\n",
      "bert.encoder.layer.0.attention.self.value.bias\n",
      "bert.encoder.layer.0.attention.output.dense.weight\n",
      "bert.encoder.layer.0.attention.output.dense.bias\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.0.intermediate.dense.weight\n",
      "bert.encoder.layer.0.intermediate.dense.bias\n",
      "bert.encoder.layer.0.output.dense.weight\n",
      "bert.encoder.layer.0.output.dense.bias\n",
      "bert.encoder.layer.0.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.attention.self.query.weight\n",
      "bert.encoder.layer.1.attention.self.query.bias\n",
      "bert.encoder.layer.1.attention.self.key.weight\n",
      "bert.encoder.layer.1.attention.self.key.bias\n",
      "bert.encoder.layer.1.attention.self.value.weight\n",
      "bert.encoder.layer.1.attention.self.value.bias\n",
      "bert.encoder.layer.1.attention.output.dense.weight\n",
      "bert.encoder.layer.1.attention.output.dense.bias\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.intermediate.dense.weight\n",
      "bert.encoder.layer.1.intermediate.dense.bias\n",
      "bert.encoder.layer.1.output.dense.weight\n",
      "bert.encoder.layer.1.output.dense.bias\n",
      "bert.encoder.layer.1.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.attention.self.query.weight\n",
      "bert.encoder.layer.2.attention.self.query.bias\n",
      "bert.encoder.layer.2.attention.self.key.weight\n",
      "bert.encoder.layer.2.attention.self.key.bias\n",
      "bert.encoder.layer.2.attention.self.value.weight\n",
      "bert.encoder.layer.2.attention.self.value.bias\n",
      "bert.encoder.layer.2.attention.output.dense.weight\n",
      "bert.encoder.layer.2.attention.output.dense.bias\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.intermediate.dense.weight\n",
      "bert.encoder.layer.2.intermediate.dense.bias\n",
      "bert.encoder.layer.2.output.dense.weight\n",
      "bert.encoder.layer.2.output.dense.bias\n",
      "bert.encoder.layer.2.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.attention.self.query.weight\n",
      "bert.encoder.layer.3.attention.self.query.bias\n",
      "bert.encoder.layer.3.attention.self.key.weight\n",
      "bert.encoder.layer.3.attention.self.key.bias\n",
      "bert.encoder.layer.3.attention.self.value.weight\n",
      "bert.encoder.layer.3.attention.self.value.bias\n",
      "bert.encoder.layer.3.attention.output.dense.weight\n",
      "bert.encoder.layer.3.attention.output.dense.bias\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.intermediate.dense.weight\n",
      "bert.encoder.layer.3.intermediate.dense.bias\n",
      "bert.encoder.layer.3.output.dense.weight\n",
      "bert.encoder.layer.3.output.dense.bias\n",
      "bert.encoder.layer.3.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.attention.self.query.weight\n",
      "bert.encoder.layer.4.attention.self.query.bias\n",
      "bert.encoder.layer.4.attention.self.key.weight\n",
      "bert.encoder.layer.4.attention.self.key.bias\n",
      "bert.encoder.layer.4.attention.self.value.weight\n",
      "bert.encoder.layer.4.attention.self.value.bias\n",
      "bert.encoder.layer.4.attention.output.dense.weight\n",
      "bert.encoder.layer.4.attention.output.dense.bias\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.intermediate.dense.weight\n",
      "bert.encoder.layer.4.intermediate.dense.bias\n",
      "bert.encoder.layer.4.output.dense.weight\n",
      "bert.encoder.layer.4.output.dense.bias\n",
      "bert.encoder.layer.4.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.attention.self.query.weight\n",
      "bert.encoder.layer.5.attention.self.query.bias\n",
      "bert.encoder.layer.5.attention.self.key.weight\n",
      "bert.encoder.layer.5.attention.self.key.bias\n",
      "bert.encoder.layer.5.attention.self.value.weight\n",
      "bert.encoder.layer.5.attention.self.value.bias\n",
      "bert.encoder.layer.5.attention.output.dense.weight\n",
      "bert.encoder.layer.5.attention.output.dense.bias\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.intermediate.dense.weight\n",
      "bert.encoder.layer.5.intermediate.dense.bias\n",
      "bert.encoder.layer.5.output.dense.weight\n",
      "bert.encoder.layer.5.output.dense.bias\n",
      "bert.encoder.layer.5.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.attention.self.query.weight\n",
      "bert.encoder.layer.6.attention.self.query.bias\n",
      "bert.encoder.layer.6.attention.self.key.weight\n",
      "bert.encoder.layer.6.attention.self.key.bias\n",
      "bert.encoder.layer.6.attention.self.value.weight\n",
      "bert.encoder.layer.6.attention.self.value.bias\n",
      "bert.encoder.layer.6.attention.output.dense.weight\n",
      "bert.encoder.layer.6.attention.output.dense.bias\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.intermediate.dense.weight\n",
      "bert.encoder.layer.6.intermediate.dense.bias\n",
      "bert.encoder.layer.6.output.dense.weight\n",
      "bert.encoder.layer.6.output.dense.bias\n",
      "bert.encoder.layer.6.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.attention.self.query.weight\n",
      "bert.encoder.layer.7.attention.self.query.bias\n",
      "bert.encoder.layer.7.attention.self.key.weight\n",
      "bert.encoder.layer.7.attention.self.key.bias\n",
      "bert.encoder.layer.7.attention.self.value.weight\n",
      "bert.encoder.layer.7.attention.self.value.bias\n",
      "bert.encoder.layer.7.attention.output.dense.weight\n",
      "bert.encoder.layer.7.attention.output.dense.bias\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.intermediate.dense.weight\n",
      "bert.encoder.layer.7.intermediate.dense.bias\n",
      "bert.encoder.layer.7.output.dense.weight\n",
      "bert.encoder.layer.7.output.dense.bias\n",
      "bert.encoder.layer.7.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.attention.self.query.weight\n",
      "bert.encoder.layer.8.attention.self.query.bias\n",
      "bert.encoder.layer.8.attention.self.key.weight\n",
      "bert.encoder.layer.8.attention.self.key.bias\n",
      "bert.encoder.layer.8.attention.self.value.weight\n",
      "bert.encoder.layer.8.attention.self.value.bias\n",
      "bert.encoder.layer.8.attention.output.dense.weight\n",
      "bert.encoder.layer.8.attention.output.dense.bias\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.intermediate.dense.weight\n",
      "bert.encoder.layer.8.intermediate.dense.bias\n",
      "bert.encoder.layer.8.output.dense.weight\n",
      "bert.encoder.layer.8.output.dense.bias\n",
      "bert.encoder.layer.8.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.attention.self.query.weight\n",
      "bert.encoder.layer.9.attention.self.query.bias\n",
      "bert.encoder.layer.9.attention.self.key.weight\n",
      "bert.encoder.layer.9.attention.self.key.bias\n",
      "bert.encoder.layer.9.attention.self.value.weight\n",
      "bert.encoder.layer.9.attention.self.value.bias\n",
      "bert.encoder.layer.9.attention.output.dense.weight\n",
      "bert.encoder.layer.9.attention.output.dense.bias\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.intermediate.dense.weight\n",
      "bert.encoder.layer.9.intermediate.dense.bias\n",
      "bert.encoder.layer.9.output.dense.weight\n",
      "bert.encoder.layer.9.output.dense.bias\n",
      "bert.encoder.layer.9.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.attention.self.query.weight\n",
      "bert.encoder.layer.10.attention.self.query.bias\n",
      "bert.encoder.layer.10.attention.self.key.weight\n",
      "bert.encoder.layer.10.attention.self.key.bias\n",
      "bert.encoder.layer.10.attention.self.value.weight\n",
      "bert.encoder.layer.10.attention.self.value.bias\n",
      "bert.encoder.layer.10.attention.output.dense.weight\n",
      "bert.encoder.layer.10.attention.output.dense.bias\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.intermediate.dense.weight\n",
      "bert.encoder.layer.10.intermediate.dense.bias\n",
      "bert.encoder.layer.10.output.dense.weight\n",
      "bert.encoder.layer.10.output.dense.bias\n",
      "bert.encoder.layer.10.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.attention.self.query.weight\n",
      "bert.encoder.layer.11.attention.self.query.bias\n",
      "bert.encoder.layer.11.attention.self.key.weight\n",
      "bert.encoder.layer.11.attention.self.key.bias\n",
      "bert.encoder.layer.11.attention.self.value.weight\n",
      "bert.encoder.layer.11.attention.self.value.bias\n",
      "bert.encoder.layer.11.attention.output.dense.weight\n",
      "bert.encoder.layer.11.attention.output.dense.bias\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.intermediate.dense.weight\n",
      "bert.encoder.layer.11.intermediate.dense.bias\n",
      "bert.encoder.layer.11.output.dense.weight\n",
      "bert.encoder.layer.11.output.dense.bias\n",
      "bert.encoder.layer.11.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.output.LayerNorm.bias\n",
      "bert.pooler.dense.weight\n",
      "bert.pooler.dense.bias\n",
      "mlp_bert.weight\n",
      "mlp_bert.bias\n",
      "mlp_len.weight\n",
      "mlp_len.bias\n",
      "mlp_days.weight\n",
      "mlp_days.bias\n",
      "mlp_exp.weight\n",
      "mlp_exp.bias\n",
      "out.weight\n",
      "out.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():                \n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c04c4dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0abf2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTHelpful(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (mlp_bert): Linear(in_features=768, out_features=50, bias=True)\n",
      "  (mlp_len): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (mlp_days): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (mlp_exp): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (out): Linear(in_features=53, out_features=5, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17871137",
   "metadata": {},
   "source": [
    "###  Optimizer & Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2efbd0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer_test = AdamW(model.parameters(),\n",
    "                           lr=3e-5,    # Default learning rate\n",
    "                           eps=1e-8    # Default epsilon value\n",
    "                          )\n",
    "\n",
    "# Total number of training steps\n",
    "total_steps = len(train_iterator) * N_EPOCHS\n",
    "    \n",
    "# Set up the learning rate scheduler\n",
    "scheduler_test = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                            num_warmup_steps=0, # Default value\n",
    "                                                            num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47eff68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    top_pred = preds.argmax(1, keepdim = True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090807ee",
   "metadata": {},
   "source": [
    "## Reproduceability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "19152263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reproduceability Set!!!!\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "set_seed(42)    # Set seed for reproducibility\n",
    "\n",
    "print(\"Reproduceability Set!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5149010a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    i = 0\n",
    "\n",
    "    #print(len(iterator))\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, text_lengths = batch.text\n",
    "        \n",
    "        exp = batch.e\n",
    "        \n",
    "        days = batch.d\n",
    "        \n",
    "        predictions = model(text, text_lengths, exp, days).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = categorical_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "        #print(i)\n",
    "\n",
    "        if i == len(iterator):\n",
    "          break\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7abf074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    i = 0\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    #print(len(iterator))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            \n",
    "            text, text_lengths = batch.text\n",
    "            \n",
    "            exp = batch.e\n",
    "        \n",
    "            days = batch.d\n",
    "            \n",
    "            predictions = model(text, text_lengths, exp, days).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = categorical_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "            i = i + 1\n",
    "\n",
    "            #print(i)\n",
    "\n",
    "            if i == len(iterator):\n",
    "              break\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b783927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1159e8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './Saved Models/Experiment#2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aaa842a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 3m 41s\n",
      "\tTrain Loss: 0.856 | Train Acc: 62.91%\n",
      "\t Val. Loss: 0.845 |  Val. Acc: 63.07%\n",
      "Epoch: 02 | Epoch Time: 3m 42s\n",
      "\tTrain Loss: 0.832 | Train Acc: 63.45%\n",
      "\t Val. Loss: 0.817 |  Val. Acc: 63.66%\n",
      "Epoch: 03 | Epoch Time: 3m 42s\n",
      "\tTrain Loss: 0.819 | Train Acc: 64.06%\n",
      "\t Val. Loss: 0.809 |  Val. Acc: 64.14%\n",
      "Epoch: 04 | Epoch Time: 3m 43s\n",
      "\tTrain Loss: 0.810 | Train Acc: 64.63%\n",
      "\t Val. Loss: 0.800 |  Val. Acc: 64.81%\n",
      "Epoch: 05 | Epoch Time: 3m 43s\n",
      "\tTrain Loss: 0.805 | Train Acc: 65.10%\n",
      "\t Val. Loss: 0.798 |  Val. Acc: 65.40%\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), save_path + 'best-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b058b351",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e376eff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "def test_categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    top_pred = preds.argmax(1, keepdim = True)\n",
    "    \n",
    "    pred_labels.append(top_pred.squeeze().tolist())\n",
    "    true_labels.append(y.squeeze().tolist())\n",
    "    \n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a41754c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    i = 0\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    #print(len(iterator))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            \n",
    "            text, text_lengths = batch.text\n",
    "            \n",
    "            exp = batch.e\n",
    "        \n",
    "            days = batch.d\n",
    "            \n",
    "            predictions = model(text, text_lengths, exp, days).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = test_categorical_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "            i = i + 1\n",
    "\n",
    "            #print(i)\n",
    "\n",
    "            if i == len(iterator):\n",
    "              break\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "31e176dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.799 | Test Acc: 65.18%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(save_path + 'best-model.pt'))\n",
    "\n",
    "test_loss, test_acc = test(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2bc92211",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_true_labels = [ item for elem in true_labels for item in elem]\n",
    "test_pred_labels = [ item for elem in pred_labels for item in elem]\n",
    "\n",
    "#print(test_true_labels)\n",
    "#print(test_true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "236fc2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE\n",
      "0.39282178217821784\n",
      "----------------------------------------------------\n",
      "MSE\n",
      "0.49084158415841583\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(\"MAE\")\n",
    "print(mean_absolute_error(test_true_labels, test_pred_labels))\n",
    "print(\"----------------------------------------------------\")\n",
    "print(\"MSE\")\n",
    "print(mean_squared_error(test_true_labels, test_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92fff29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
